# 1. 训练dqn policy
# 2. 测试dqn，并输出state-action数据dqn_policy_i，作为专家数据
# 3. 训练gail policy
# 4. 测试gail，并输出state-action数据gail_policy_i
# 5. 对比dqn_policy_i和gail_policy_i中动作的使用分布

# parameters
        EpRewMean: reward giver(GAN)给的奖励值
    EpTrueRewMean: 自定义的奖励值的平均数，越大越好
     d_expert_acc: 判别器对专家策略判断为真的概率大于50%，最后接近于1
  d_generator_acc: 判别器对交互策略判断为真的概率小于50%，最后接近于0
    d_expert_loss:
 d_generator_loss:
   d_entropy_loss: 越小越好
        d_entropy: 越大越好
  ev_tdlam_before:
        g_entloss: 新策略的熵损失（新策略的的熵和系数）
        g_entropy: 新策略的熵
        g_mean_kl: 越小越好
      g_optimgain: TRPO的目标函数
       g_surrgain: TRPO的目标函数(新就策略的比例ratio和优势函数atarg)